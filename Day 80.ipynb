{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Packt Computer Vision with Python3 - Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amira\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import Liraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from sklearn import  svm, metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [13.],\n",
       "       [ 9.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [10.],\n",
       "       [15.],\n",
       "       [ 5.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [15.],\n",
       "       [ 2.],\n",
       "       [ 0.],\n",
       "       [11.],\n",
       "       [ 8.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 4.],\n",
       "       [12.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 8.],\n",
       "       [ 8.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [ 8.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 9.],\n",
       "       [ 8.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 4.],\n",
       "       [11.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [12.],\n",
       "       [ 7.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [14.],\n",
       "       [ 5.],\n",
       "       [10.],\n",
       "       [12.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 6.],\n",
       "       [13.],\n",
       "       [10.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what kind of data do we already have?\n",
    "from sklearn import datasets\n",
    "digits=datasets.load_digits()\n",
    "\n",
    "\n",
    "example_image=digits.images[0]\n",
    "print(type(example_image))\n",
    "plt.imshow(example_image); plt.show()\n",
    "example_image.reshape((8*8,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-0e8d17c2dda7>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\amira\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\amira\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\amira\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\amira\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\amira\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#acquire standard MNIST handwritten digit data\n",
    "#http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "data_dir = '/tmp/tensorflow/mnist/input_data'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now we load and examine the data\n",
    "train_data=mnist.train.images\n",
    "print(train_data.shape)\n",
    "n_samples = train_data.shape[0]\n",
    "\n",
    "\n",
    "train_labels=np.array(np.where(mnist.train.labels==1))[1]\n",
    "\n",
    "plt.imshow(train_data[1000].reshape((28,28))); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "# Learn about gamma and other SVM parameters here:\n",
    "# http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\n",
    "# Exercise:  Experiment with the parameters to see how they affect execution \n",
    "#             time and accuracy\n",
    "\n",
    "# Train the model -- we're only going to use the training data (and not\n",
    "# the test data) to ensure that our model generalizes to unseen cases.\n",
    "# This (training) is typically what takes the most computational time\n",
    "# when doing machine learning.\n",
    "classifier.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict the value of the digit on the test data:\n",
    "test_data=mnist.test.images\n",
    "test_labels=np.array(np.where(mnist.test.labels==1))[1]\n",
    "\n",
    "expected = test_labels\n",
    "predicted = classifier.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And display the results\n",
    "print(\"See https://en.wikipedia.org/wiki/Precision_and_recall to understand metric definitions\")\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(test_data, predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":\n",
    "#Let's test our model on images we draw ourselves!\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "%pylab notebook \n",
    "#This is needed for plot widgets\n",
    "\n",
    "class Annotator(object):\n",
    "    def __init__(self, axes):\n",
    "        self.axes = axes\n",
    "\n",
    "        self.xdata = []\n",
    "        self.ydata = []\n",
    "        self.xy    = []\n",
    "        self.drawon = False\n",
    "\n",
    "    def mouse_move(self, event):\n",
    "        if not event.inaxes:\n",
    "            return\n",
    "\n",
    "        x, y = event.xdata, event.ydata\n",
    "        if self.drawon:\n",
    "            self.xdata.append(x)\n",
    "            self.ydata.append(y)\n",
    "            self.xy.append((int(x),int(y)))\n",
    "            line = Line2D(self.xdata,self.ydata)\n",
    "            line.set_color('r')\n",
    "            self.axes.add_line(line)\n",
    "\n",
    "            plt.draw()\n",
    "\n",
    "    def mouse_release(self, event):\n",
    "        # Erase x and y data for new line\n",
    "        self.xdata = []\n",
    "        self.ydata = []\n",
    "        self.drawon = False\n",
    "        \n",
    "    def mouse_press(self, event):\n",
    "        self.drawon = True\n",
    "\n",
    "\n",
    "img = np.zeros((28,28,3),dtype='uint8')\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(3,3))\n",
    "axes.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.gray()\n",
    "annotator = Annotator(axes)\n",
    "plt.connect('motion_notify_event', annotator.mouse_move)\n",
    "plt.connect('button_release_event', annotator.mouse_release)\n",
    "plt.connect('button_press_event', annotator.mouse_press)\n",
    "\n",
    "axes.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we see how our model \"sees\" (predicts the digit from)\n",
    "# our hand drawn image...\n",
    "# First, we rasterize (convert to pixels) our vector data\n",
    "# and process the image to more closely resemble something\n",
    "# drawn with a pencil or pressure-sensitive tablet.\n",
    "\n",
    "digimg = np.zeros((28,28,3),dtype='uint8')\n",
    "for ind, points in enumerate(annotator.xy[:-1]):\n",
    "    digimg=cv2.line(digimg, annotator.xy[ind], annotator.xy[ind+1],(255,0,0),1)\n",
    "digimg = cv2.GaussianBlur(digimg,(5,5),1.0)\n",
    "digimg = (digimg.astype('float') *1.0/np.amax(digimg)).astype('float')[:,:,0]\n",
    "digimg **= 0.5; digimg[digimg>0.9]=1.0\n",
    "\n",
    "#The model is expecting the input in a particular format\n",
    "testim = digimg.reshape((-1,28*28))\n",
    "\n",
    "print(\"Support vector machine prediction:\",classifier.predict( testim ))\n",
    "\n",
    "outimg = testim.reshape((28,28))\n",
    "figure(figsize=(3,3)); imshow(outimg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data (if not already done, or just in case)\n",
    "data_dir = '/tmp/tensorflow/mnist/input_data'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "  tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "# Train\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Model accuracy:\",sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                  y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test again on hand drawn images\n",
    "img = np.zeros((28,28,3),dtype='uint8')\n",
    "fig, axes = plt.subplots(figsize=(3,3))\n",
    "axes.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.gray()\n",
    "annotator = Annotator(axes)\n",
    "plt.connect('motion_notify_event', annotator.mouse_move)\n",
    "plt.connect('button_release_event', annotator.mouse_release)\n",
    "plt.connect('button_press_event', annotator.mouse_press)\n",
    "axes.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rasterize and preprocess\n",
    "digimg = np.zeros((28,28,3),dtype='uint8')\n",
    "for ind, points in enumerate(annotator.xy[:-1]):\n",
    "    digimg=cv2.line(digimg, annotator.xy[ind], annotator.xy[ind+1],(255,0,0),1)\n",
    "digimg = cv2.GaussianBlur(digimg,(5,5),1.0)\n",
    "digimg = (digimg.astype('float') *1.0/np.amax(digimg)).astype('float')[:,:,0]\n",
    "digimg **= 0.5; digimg[digimg>0.9]=1.0\n",
    "testim = digimg.reshape((-1,28*28))\n",
    "\n",
    "#and run through our trained Softmax model\n",
    "for tindex in range(10):\n",
    "    testlab = np.zeros((1,10))\n",
    "    testlab[0,tindex] = 1\n",
    "    if sess.run(accuracy, feed_dict={x: testim, y_ : testlab}) == 1:\n",
    "        break\n",
    "print(\"Predicted #:\",tindex) #tindex = TF model prediction\n",
    "\n",
    "#display the rasterized image\n",
    "outimg = testim.reshape((28,28))\n",
    "figure(figsize=(3,3)); imshow(outimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deepnn(x):\n",
    "  \"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "  Args:\n",
    "    x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "    number of pixels in a standard MNIST image.\n",
    "  Returns:\n",
    "    A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "    equal to the logits of classifying the digit into one of 10 classes (the\n",
    "    digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "    dropout.\n",
    "  \"\"\"\n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "  with tf.name_scope('reshape'):\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "  with tf.name_scope('conv1'):\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "  # Pooling layer - downsamples by 2X.\n",
    "  with tf.name_scope('pool1'):\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "  # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "  with tf.name_scope('conv2'):\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "     # Second pooling layer.\n",
    "  with tf.name_scope('pool2'):\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "  with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "  # features.\n",
    "  with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "  # Map the 1024 features to 10 classes, one for each digit\n",
    "  with tf.name_scope('fc2'):\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "  return y_conv, keep_prob\n",
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "###begin main code\n",
    "\n",
    "data_dir= '/tmp/tensorflow/mnist/input_data'\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Build the graph for the deep net\n",
    "y_conv, keep_prob = deepnn(x)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n",
    "                                                        logits=y_conv)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "graph_location = tempfile.mkdtemp()\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph())\n",
    "# Let's run the model\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i % 100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "# How did we do?\n",
    "print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rasterize and preprocess the above\n",
    "digimg = np.zeros((28,28,3),dtype='uint8')\n",
    "for ind, points in enumerate(annotator.xy[:-1]):\n",
    "    digimg=cv2.line(digimg, annotator.xy[ind], annotator.xy[ind+1],(255,0,0),1)\n",
    "digimg = cv2.GaussianBlur(digimg,(5,5),1.0)\n",
    "digimg = (digimg.astype('float') *1.0/np.amax(digimg)).astype('float')[:,:,0]\n",
    "digimg **= 0.5; digimg[digimg>0.9]=1.0\n",
    "testim = digimg.reshape((-1,28*28))\n",
    "\n",
    "# And run through our model\n",
    "for tindex in range(10):\n",
    "    testlab = np.zeros((1,10))\n",
    "    testlab[0,tindex] = 1\n",
    "    if accuracy.eval(feed_dict={x: testim, y_: testlab, \n",
    "                                keep_prob: 1.0}) == 1:\n",
    "        break\n",
    "\n",
    "print(\"Predicted #:\",tindex) #tindex = TF model prediction\n",
    "\n",
    "# Display our rasterized digit\n",
    "outimg = testim.reshape((28,28))\n",
    "figure(figsize=(3,3)); imshow(outimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
