{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System for Movies in Pytorch using Stack Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the dataset\n",
    "movies = pd.read_csv('dataset\\ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('dataset\\ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings = pd.read_csv('dataset\\ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training set and the test set\n",
    "training_set = pd.read_csv('dataset\\ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('dataset\\ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of users and movies\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the data into array with users in lines and movies in column\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users +1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users ]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users ]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies -1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data into torch tensor\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Building the Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the architecture of the Stack Auto Encoder Neural Network\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Training and Testing Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.7716305255771234\n",
      "epoch: 2 loss: 1.0967801084957054\n",
      "epoch: 3 loss: 1.0534703672820198\n",
      "epoch: 4 loss: 1.0382487616912517\n",
      "epoch: 5 loss: 1.0310048144190709\n",
      "epoch: 6 loss: 1.0264577108558546\n",
      "epoch: 7 loss: 1.023866060156085\n",
      "epoch: 8 loss: 1.0218215750076067\n",
      "epoch: 9 loss: 1.0206260127129247\n",
      "epoch: 10 loss: 1.0196243124424016\n",
      "epoch: 11 loss: 1.0188193786177713\n",
      "epoch: 12 loss: 1.0184607558843952\n",
      "epoch: 13 loss: 1.0177125568089451\n",
      "epoch: 14 loss: 1.0174462516876264\n",
      "epoch: 15 loss: 1.0170087667233798\n",
      "epoch: 16 loss: 1.016699957587575\n",
      "epoch: 17 loss: 1.016782234251401\n",
      "epoch: 18 loss: 1.0166936734035497\n",
      "epoch: 19 loss: 1.0163385860225136\n",
      "epoch: 20 loss: 1.0160661608217583\n",
      "epoch: 21 loss: 1.016066423249105\n",
      "epoch: 22 loss: 1.0159179550592885\n",
      "epoch: 23 loss: 1.0159481700236892\n",
      "epoch: 24 loss: 1.015820553539149\n",
      "epoch: 25 loss: 1.0153496082356233\n",
      "epoch: 26 loss: 1.0156935108095215\n",
      "epoch: 27 loss: 1.0154045487002659\n",
      "epoch: 28 loss: 1.0148520554117353\n",
      "epoch: 29 loss: 1.014002339456173\n",
      "epoch: 30 loss: 1.0119312736343729\n",
      "epoch: 31 loss: 1.0101338304595684\n",
      "epoch: 32 loss: 1.0095809532755546\n",
      "epoch: 33 loss: 1.0052905717164\n",
      "epoch: 34 loss: 1.0060609148683102\n",
      "epoch: 35 loss: 1.001741096214346\n",
      "epoch: 36 loss: 1.000407226587404\n",
      "epoch: 37 loss: 0.996548774507933\n",
      "epoch: 38 loss: 0.9959479207748211\n",
      "epoch: 39 loss: 0.9928609034153066\n",
      "epoch: 40 loss: 0.9928842774061782\n",
      "epoch: 41 loss: 0.9893463051283838\n",
      "epoch: 42 loss: 0.9883175701737158\n",
      "epoch: 43 loss: 0.9847028392717398\n",
      "epoch: 44 loss: 0.9843392672743871\n",
      "epoch: 45 loss: 0.9823344745207653\n",
      "epoch: 46 loss: 0.9828718354178717\n",
      "epoch: 47 loss: 0.9755068037183467\n",
      "epoch: 48 loss: 0.976049590805783\n",
      "epoch: 49 loss: 0.9723808976803795\n",
      "epoch: 50 loss: 0.9752180406154324\n",
      "epoch: 51 loss: 0.9723876675027489\n",
      "epoch: 52 loss: 0.982120798884341\n",
      "epoch: 53 loss: 0.9758690679152722\n",
      "epoch: 54 loss: 0.9809275029559045\n",
      "epoch: 55 loss: 0.9767805317940541\n",
      "epoch: 56 loss: 0.9717441031758917\n",
      "epoch: 57 loss: 0.9666036761403747\n",
      "epoch: 58 loss: 0.9652281076049499\n",
      "epoch: 59 loss: 0.9638693476589678\n",
      "epoch: 60 loss: 0.9644398591868222\n",
      "epoch: 61 loss: 0.9607724953770616\n",
      "epoch: 62 loss: 0.9640653156188371\n",
      "epoch: 63 loss: 0.9589529326142794\n",
      "epoch: 64 loss: 0.9552770574210728\n",
      "epoch: 65 loss: 0.9548035180254774\n",
      "epoch: 66 loss: 0.9547390616519015\n",
      "epoch: 67 loss: 0.9526346822172648\n",
      "epoch: 68 loss: 0.9527261634348582\n",
      "epoch: 69 loss: 0.950030483063566\n",
      "epoch: 70 loss: 0.9502256515234518\n",
      "epoch: 71 loss: 0.947617106032022\n",
      "epoch: 72 loss: 0.9476493501952578\n",
      "epoch: 73 loss: 0.9467796589426811\n",
      "epoch: 74 loss: 0.9471238661169186\n",
      "epoch: 75 loss: 0.9448288800877404\n",
      "epoch: 76 loss: 0.9454277818942479\n",
      "epoch: 77 loss: 0.9420771744214584\n",
      "epoch: 78 loss: 0.9443801303520836\n",
      "epoch: 79 loss: 0.94192748894762\n",
      "epoch: 80 loss: 0.9417015622686106\n",
      "epoch: 81 loss: 0.9397019238852027\n",
      "epoch: 82 loss: 0.9402720783362699\n",
      "epoch: 83 loss: 0.9383019634891889\n",
      "epoch: 84 loss: 0.9389777958978839\n",
      "epoch: 85 loss: 0.9380023125637312\n",
      "epoch: 86 loss: 0.9380546620093633\n",
      "epoch: 87 loss: 0.936305850088426\n",
      "epoch: 88 loss: 0.9372004162913793\n",
      "epoch: 89 loss: 0.9364648512932732\n",
      "epoch: 90 loss: 0.9373275341509896\n",
      "epoch: 91 loss: 0.9352388940991857\n",
      "epoch: 92 loss: 0.9357720134737606\n",
      "epoch: 93 loss: 0.9346743804623973\n",
      "epoch: 94 loss: 0.9353356305152412\n",
      "epoch: 95 loss: 0.9342322574922384\n",
      "epoch: 96 loss: 0.9338328901528901\n",
      "epoch: 97 loss: 0.9330254413086467\n",
      "epoch: 98 loss: 0.9332171646322696\n",
      "epoch: 99 loss: 0.9328656208508083\n",
      "epoch: 100 loss: 0.9322210583062569\n",
      "epoch: 101 loss: 0.9321194875357964\n",
      "epoch: 102 loss: 0.9318422878243254\n",
      "epoch: 103 loss: 0.9317098194006693\n",
      "epoch: 104 loss: 0.9313580602045567\n",
      "epoch: 105 loss: 0.9312476148972207\n",
      "epoch: 106 loss: 0.9305370557656953\n",
      "epoch: 107 loss: 0.9299057593115122\n",
      "epoch: 108 loss: 0.9306347543809388\n",
      "epoch: 109 loss: 0.9297489099098866\n",
      "epoch: 110 loss: 0.9296842125972307\n",
      "epoch: 111 loss: 0.9291060406320122\n",
      "epoch: 112 loss: 0.9296055836344427\n",
      "epoch: 113 loss: 0.9287789423558768\n",
      "epoch: 114 loss: 0.9286616624695712\n",
      "epoch: 115 loss: 0.9275820124299018\n",
      "epoch: 116 loss: 0.9281322258562135\n",
      "epoch: 117 loss: 0.9277220591376686\n",
      "epoch: 118 loss: 0.9283664506144041\n",
      "epoch: 119 loss: 0.927403285029704\n",
      "epoch: 120 loss: 0.9275592321665824\n",
      "epoch: 121 loss: 0.9266513330688494\n",
      "epoch: 122 loss: 0.9271666307241517\n",
      "epoch: 123 loss: 0.9260314159416607\n",
      "epoch: 124 loss: 0.9268209403112765\n",
      "epoch: 125 loss: 0.925623607124668\n",
      "epoch: 126 loss: 0.9259999271838472\n",
      "epoch: 127 loss: 0.9247740748996058\n",
      "epoch: 128 loss: 0.9258122347331101\n",
      "epoch: 129 loss: 0.9248359655731231\n",
      "epoch: 130 loss: 0.9249249698676485\n",
      "epoch: 131 loss: 0.9243264833291961\n",
      "epoch: 132 loss: 0.9246373302157631\n",
      "epoch: 133 loss: 0.9237693986159963\n",
      "epoch: 134 loss: 0.9240911113924563\n",
      "epoch: 135 loss: 0.9234131066412946\n",
      "epoch: 136 loss: 0.9237954160071732\n",
      "epoch: 137 loss: 0.9229634936204357\n",
      "epoch: 138 loss: 0.9235494040253324\n",
      "epoch: 139 loss: 0.9227115479330367\n",
      "epoch: 140 loss: 0.9231479058609922\n",
      "epoch: 141 loss: 0.9220489773269585\n",
      "epoch: 142 loss: 0.9226478461370149\n",
      "epoch: 143 loss: 0.921670012853688\n",
      "epoch: 144 loss: 0.9221969876776152\n",
      "epoch: 145 loss: 0.9212141338030965\n",
      "epoch: 146 loss: 0.9216801987712743\n",
      "epoch: 147 loss: 0.9211444025091078\n",
      "epoch: 148 loss: 0.9212797746609084\n",
      "epoch: 149 loss: 0.9204750188613642\n",
      "epoch: 150 loss: 0.920693114106069\n",
      "epoch: 151 loss: 0.9203784967725274\n",
      "epoch: 152 loss: 0.9204368293704145\n",
      "epoch: 153 loss: 0.9195395836930886\n",
      "epoch: 154 loss: 0.9199056333576835\n",
      "epoch: 155 loss: 0.9193068173770862\n",
      "epoch: 156 loss: 0.9198123585963918\n",
      "epoch: 157 loss: 0.9189705211678927\n",
      "epoch: 158 loss: 0.919301603484228\n",
      "epoch: 159 loss: 0.9187879679606005\n",
      "epoch: 160 loss: 0.9189827534536557\n",
      "epoch: 161 loss: 0.918228626891366\n",
      "epoch: 162 loss: 0.9183083671189944\n",
      "epoch: 163 loss: 0.9178032064495225\n",
      "epoch: 164 loss: 0.9182223802998023\n",
      "epoch: 165 loss: 0.9181448923983421\n",
      "epoch: 166 loss: 0.9177102451990565\n",
      "epoch: 167 loss: 0.9170183099397344\n",
      "epoch: 168 loss: 0.9177355504669069\n",
      "epoch: 169 loss: 0.9170490307614099\n",
      "epoch: 170 loss: 0.9175716151053267\n",
      "epoch: 171 loss: 0.9168493656070242\n",
      "epoch: 172 loss: 0.9170144125659759\n",
      "epoch: 173 loss: 0.9164989879344432\n",
      "epoch: 174 loss: 0.9170106180198289\n",
      "epoch: 175 loss: 0.9162865151209614\n",
      "epoch: 176 loss: 0.9166912458128925\n",
      "epoch: 177 loss: 0.9160690667203075\n",
      "epoch: 178 loss: 0.9163185147853976\n",
      "epoch: 179 loss: 0.9155254445291533\n",
      "epoch: 180 loss: 0.915729482615929\n",
      "epoch: 181 loss: 0.915599248761417\n",
      "epoch: 182 loss: 0.9157294099262501\n",
      "epoch: 183 loss: 0.9155556913893778\n",
      "epoch: 184 loss: 0.9154127300915361\n",
      "epoch: 185 loss: 0.9149827509163391\n",
      "epoch: 186 loss: 0.9152785706112133\n",
      "epoch: 187 loss: 0.914638161448138\n",
      "epoch: 188 loss: 0.9152508416100847\n",
      "epoch: 189 loss: 0.9146168885544691\n",
      "epoch: 190 loss: 0.9147660059418204\n",
      "epoch: 191 loss: 0.9141583450518319\n",
      "epoch: 192 loss: 0.9142433392233843\n",
      "epoch: 193 loss: 0.9138083657650562\n",
      "epoch: 194 loss: 0.9142191703572864\n",
      "epoch: 195 loss: 0.9138923813569638\n",
      "epoch: 196 loss: 0.9138933700776383\n",
      "epoch: 197 loss: 0.9134400534402428\n",
      "epoch: 198 loss: 0.9139668655087929\n",
      "epoch: 199 loss: 0.9133790329946464\n",
      "epoch: 200 loss: 0.9133712916189766\n"
     ]
    }
   ],
   "source": [
    "# Training the SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9478524766918428\n"
     ]
    }
   ],
   "source": [
    "# Testing the SAE\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user])\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "        s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
